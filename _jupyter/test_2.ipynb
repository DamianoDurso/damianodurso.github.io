{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of the blog post\n",
    "\n",
    "Situation\n",
    "- self-report measures are one of the most used tools to evaluate peoples' opinion beliefs needs, cognitive abilities etc\n",
    "\n",
    "Complication\n",
    "Developing good measures is \n",
    "- Expense and Resources: Detail the cost implications, including the need for large sample sizes and extensive data collection efforts.\n",
    "- Time Consumption: Elaborate on the time required to review existing literature, develop questions, pilot tests, and analyze data.\n",
    "- Risk of Failure:Discuss the risks of poorly designed measures, including respondent fatigue, biased answers, and unreliable data.\n",
    "- Limitations in Testing Explain the practical limitations of testing numerous questions and variations, and how this increases the risk of introducing biases.\n",
    "\n",
    "Question\n",
    "How can we speed-up and simplify scale development so that, when we use self-report measures, we are quite sure they work well?\n",
    "\n",
    "Solution\n",
    "Embedding psychometrics!\n",
    "What is embedding psychometrics:....\n",
    "    and what is sentence embeddings to begin with...\n",
    "\n",
    "- Embedding psychometrics is cheap: it does not require extensive data collection, large sample size as it is simply done through leveraging the power of pre-trained llms\n",
    "- Embedding psychometrics is fast: surely questions need to be reviewed and collected, but one can simply pilot existing questions all at once and analyze the data in real time without needing to wait for data collection.\n",
    "- Embedding psychometrics fails fast\n",
    "- Embedding psychometrics not bias free but less bias sensitive \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is embedding Psychometrics\n",
    "    How is it different from classic psychometrics\n",
    "\n",
    "Why embedding psychometrics?\n",
    "\n",
    "How? \n",
    "\n",
    "\n",
    "Classic Psychometrics\n",
    "- Research question\n",
    "Embedding Psychometrics\n",
    "\n",
    "\n",
    "Structure of the blopost\n",
    "- E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are my employees engaged? What's customers' opinion about our proudct? Are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surveying all the way\n",
    "**How many times have you used a survey to determine your employees' engagement? Or customers' satisfaction?** And how about using tests to assess someone's personality or cognitive abilities to determine if they are a suitable candidate for a program/job etc? **Most companies/institutions use survey or tests as data-sources to draw insights and make data-driven decisions.**\n",
    "\n",
    "These measurement tools are not perfect. They require careful piloting, calibration and psychometric analyses to ensure that the data we gather is reliable and valid. The risks are often too high to bear for using bad measures, such as... find examples..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes, however, we do not have the time or resources to develop proper measurement tools because:\n",
    "\n",
    "- **They require lots of money and resources**. Surely survey tools are cheap, but the expertise required to develop good questions, evaluate their fit to the theme/construct one wants to measure is high. Also, one needs to collect large data sets to validate the survey and determine its accuracy. Also, Developing good questions takes time and, in the best case scenario, lots of qualitative and scientific literature investigation;\n",
    "- **There is a high risk of failure**: Measurement experts know too well that scale development is almost an art, which that requires lots of trial and errors. For instance, formulating questions positvely (\"I like parties\") or negatively (\"I hate parties\") can trigger different response processes. And, while one may want to try all possible permutations of a question, this will increase the risk of respondents' fatigue and reduced motivation when filling out a survey, which will result in low quality data and an increased risk in response biases (e.g. social desirability or carless responding);\n",
    "- and much more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, one may simply ask, is it really not possible to simplify scale development and assessment and make it less resource intensive? The short answer is \"Kinda, and you are in the right page to discover how\"!. \n",
    "\n",
    "Large Language Model embeddings psychometrics (LLMEP), is a new and exiting area of research which can help streamlining and speeding up scale development like never before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post, I will:\n",
    "- Describe LLMEP and explain how it works\n",
    "- Show how we can use LLMEP \n",
    "- Close off with some remarks and workd of caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The secrets of LLMEP\n",
    "\n",
    "- Small intro to NLP and Transformers\n",
    "    - NLP\n",
    "        - Definition: process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning.\n",
    "        - The advent of transformers: NLP has gone a long way in understanding human language through the use of increasingly complex model. But the biggest revolution was the one from recurrent neural networks to transformers. Recurrent neural networks processed language sequentially, token (e.g., word) by token. Since the revolutionary paper \" Attention is all you need\" transformers have become the golden standards by capturing context efficiently. Compared to previous models transformers were able to:\n",
    "        - processing text in parallel\n",
    "        - capture long text dependencies through context\n",
    "    - Transformers \n",
    "        - Word-based transformers\n",
    "        - Sentence-based transformers\n",
    "- What are embeddings\n",
    "    -  Embeddings \n",
    "- How can we use embeddings for scale development\n",
    "    - The unexpected twist \"using sentence embeddings as response vectors\" [ Mata and Guenole] \n",
    "\n",
    "Large Language Models are one of the most exciting scientific innovation of the last decade and are becoming ubiquitous, from chatbots (chatGPT) to..... LLMs are defined as  a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification (wiki)\n",
    "\n",
    "Most of these models are based on a transformers architecture, which comprises an encoder and a decoder.\n",
    "\n",
    " The encoder processes an input sequence of tokenized\n",
    "text, by repeatedly applying attention mechanisms that help the model capture contextual\n",
    "relationships in the data. This results in a condensed vector representation of the input\n",
    "sequence. The decoder then takes this representation and, using its own layers of attention to\n",
    "the encoder's output, predicts the desired output sequence.\n",
    "\n",
    "Encoder models, also known as bidirectional transformer models, were first\n",
    "popularized with the Bidirectional Encoder Representations from Transformers-model\n",
    "(BERT; Devlin et al., 2018), and proved to excel at a variety of linguistic challenges. The\n",
    "pre-training of the original BERT involved two training objectives: Masked language\n",
    "modeling (MLM) and next sentence prediction (NSP). In MLM, parts of a sequence of text\n",
    "(e.g., sentence) in the training data are intentionally obscured by a masking algorithm. The\n",
    "modelsâ€™ objective is then to correctly identify the masked tokens. In NSP, pairs of sentences\n",
    "are extracted from the training corpus. For each original sentence pair, a secondary version is\n",
    "created by replacing the second sentence with a randomly selected one. During training, the\n",
    "model must determine if the second sentence in a pair genuinely follows the first or has been\n",
    "randomly inserted.\n",
    "\n",
    "LLMEP \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
