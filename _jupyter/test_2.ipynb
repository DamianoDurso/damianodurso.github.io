{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's going to be in this blogpost\n",
    "\n",
    "- Describe the issue with standard survey development\n",
    "- Introduce the idea of embeddings psychometrics\n",
    "    - Create text (items) embeddings\n",
    "    - Perform Psycometric analysis\n",
    "- Introduce step 1\n",
    "- Cliff-hanger to step 2\n",
    "- Close off with some remarks and workd of caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The workhorse of modern people listening\n",
    "\n",
    "**How many times have you used a survey to determine your employees' engagement? Or customers' satisfaction?** And how about using tests to assess someone's personality or cognitive abilities to determine if they are a suitable candidate for a program/job etc? **Most companies/institutions use survey or tests as data-sources to draw insights and make data-driven decisions.**\n",
    "\n",
    "These measurement tools are not perfect. They require careful piloting, calibration and psychometric analyses to ensure that the gathered data is reliable and valid. The risks are often too high to bear for using bad measures, such as... find examples...\n",
    "\n",
    "Oftentimes, however, there is a lack of time or resources to develop proper measurement tools since:\n",
    "\n",
    "- **They require lots of money and resources**. Surely survey tools are cheap, but the expertise required to develop good questions, evaluate their fit to the theme/construct one wants to measure is high. Also, one needs to collect large data sets to validate the survey and determine its accuracy. Also, Developing good questions takes time and, in the best case scenario, lots of qualitative and scientific literature investigation;\n",
    "- **There is a high risk of failure**: Measurement experts know too well that scale development is almost an art, which that requires lots of trial and errors. For instance, formulating questions positvely (\"I like parties\") or negatively (\"I hate parties\") can trigger different response processes. And, while one may want to try all possible permutations of a question, this will increase the risk of respondents' fatigue and reduced motivation when filling out a survey, which will result in low quality data and an increased risk in response biases (e.g. social desirability or carless responding);\n",
    "- and much more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doping the workhorse\n",
    "\n",
    "So, one may simply ask, is it really not possible to simplify scale development and assessment and make it less resource intensive? The short answer is \"Kinda, and you are in the right page to discover how\"!. \n",
    "\n",
    "Large Language Model embeddings psychometrics (LLMEP), is a new and exiting area of research which can help streamlining and speeding up scale development like never before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It all began with an LLM\n",
    "Large Language Models are one of the most exciting scientific innovation of the last decade and are becoming ubiquitous, from chatbots (chatGPT) to..... LLMs are defined as  a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification, next word prediction etc.(wiki)\n",
    "\n",
    "These models can have different type of architectures depending on the task they are developed to. For instance, if one wants to produce text a decoder model would preferred (such as GPT). For LLMEP we will be focusing mostly on analyze text, which is better achieved using encoder models. You can think of an encoder as a transformation tool that takes text and transforms it into a multidimensional numerical vector, also called embedding. (maybe add text about the transformation to explain more clearly) [encoder and a decoder layer](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)\n",
    "\n",
    "The most popular architecture for encoding models is that of Bidirectional Encoder Representations from Transformers ([BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)). BERT models are exceptionally good at understanding words due to their ability to capture word context by \"looking\" at the text both to the left and right of the word. However, when dealing with questions from a survey, which consist of entire sentences rather than single words, we need models that can transform and analyze full sentences. This is where Sentence-BERT (SBERT) comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with Traditional Sentence Embeddings\n",
    "\n",
    "Before SBERT, one had to aggregate word-level embeddings (typically through max or mean pooling) to obtain sentence embeddings. The problem with this approach is that capturing the semantic meaning of sentences can be challenging. For instance, simple averaging might miss subtle nuances in sentence meaning.\n",
    "\n",
    "Let's start by demonstrating how to compute word embeddings using a BERT model and then perform mean pooling to obtain sentence embeddings.\n",
    "\n",
    "**Compute Word Embeddings with BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings Shape (Sentence 1): torch.Size([13, 768])\n",
      "Word Embeddings Shape (Sentence 2): torch.Size([13, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"Embedding psychometrics is important for better survey analysis.\"\n",
    "sentence2 = \"Embedding psychometrics enhances the reliability of surveys.\"\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt', add_special_tokens=True)\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# Get word embeddings (excluding special tokens)\n",
    "word_embeddings1 = outputs1.last_hidden_state.squeeze(0)[1:-1]\n",
    "word_embeddings2 = outputs2.last_hidden_state.squeeze(0)[1:-1]\n",
    "\n",
    "print(\"Word Embeddings Shape (Sentence 1):\", word_embeddings1.shape)\n",
    "print(\"Word Embeddings Shape (Sentence 2):\", word_embeddings2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Pooling to Obtain Sentence Embeddings\n",
    "\n",
    "To obtain a sentence embedding from the word embeddings, we can use mean pooling. This technique averages the embeddings of all tokens in the sentence to create a single vector representing the entire sentence.\n",
    "\n",
    "**Compute Sentence Embeddings Using Mean Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding (Mean Pooling) Shape (Sentence 1): (768,)\n",
      "Sentence Embedding (Mean Pooling) Shape (Sentence 2): (768,)\n"
     ]
    }
   ],
   "source": [
    "# Mean pooling of word embeddings to get the sentence embeddings\n",
    "sentence_embedding_mean_pooling1 = word_embeddings1.mean(dim=0).numpy()\n",
    "sentence_embedding_mean_pooling2 = word_embeddings2.mean(dim=0).numpy()\n",
    "\n",
    "print(\"Sentence Embedding (Mean Pooling) Shape (Sentence 1):\", sentence_embedding_mean_pooling1.shape)\n",
    "print(\"Sentence Embedding (Mean Pooling) Shape (Sentence 2):\", sentence_embedding_mean_pooling2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Sentence-BERT (SBERT)\n",
    "SBERT models learn the semantic meaning of sentences more effectively by leveraging [siamese and triplet networks () in its architecture](https://towardsdatascience.com/sbert-deb3d4aef8a4). These models significantly improve the ability to capture the meaning of entire sentences, such as those in survey questions, allowing us to transform them into item embeddings that can be used for psychometric analyses.\n",
    "\n",
    "Compute Sentence Embeddings Using SBERT\n",
    "\n",
    "Let's compute sentence embeddings using SBERT and compare them with the mean-pooled embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Sentence Embedding Shape (Sentence 1): (768,)\n",
      "SBERT Sentence Embedding Shape (Sentence 2): (768,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Compute sentence embeddings using SBERT for both sentences\n",
    "sbert_sentence_embedding1 = sbert_model.encode(sentence1)\n",
    "sbert_sentence_embedding2 = sbert_model.encode(sentence2)\n",
    "\n",
    "print(\"SBERT Sentence Embedding Shape (Sentence 1):\", sbert_sentence_embedding1.shape)\n",
    "print(\"SBERT Sentence Embedding Shape (Sentence 2):\", sbert_sentence_embedding2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing BERT with SBERT\n",
    "\n",
    "Finally, let's compare the similarity between the two sentences using both the mean-pooled word-level embeddings and the SBERT sentence-level embeddings. This comparison will illustrate how much more effectively SBERT captures the semantic relationship between sentences.\n",
    "\n",
    "**Compute Cosine Similarity Between Sentence Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (Mean-Pooling BERT): 0.9296162724494934\n",
      "Cosine Similarity (SBERT): 0.7960948348045349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between the mean-pooled word-level embeddings\n",
    "similarity_mean_pooling = cosine_similarity(\n",
    "    [sentence_embedding_mean_pooling1],\n",
    "    [sentence_embedding_mean_pooling2]\n",
    ")[0][0]\n",
    "\n",
    "# Compute cosine similarity between SBERT sentence embeddings\n",
    "similarity_sbert = cosine_similarity(\n",
    "    [sbert_sentence_embedding1],\n",
    "    [sbert_sentence_embedding2]\n",
    ")[0][0]\n",
    "\n",
    "print(f\"Cosine Similarity (Mean-Pooling BERT): {similarity_mean_pooling}\")\n",
    "print(f\"Cosine Similarity (SBERT): {similarity_sbert}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "SBERT-based models capture the meaning of sentences much better than traditional BERT-based models using mean pooling. This allows us to transform entire sentences, such as items in a questionnaire, into more accurate item embeddings for psychometric analyses. By embedding psychometrics, we can make survey analysis more robust, efficient, and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The secrets of LLMEP\n",
    "\n",
    "- Small intro to NLP and Transformers\n",
    "    - NLP\n",
    "        - Definition: process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning.\n",
    "        - The advent of transformers: NLP has gone a long way in understanding human language through the use of increasingly complex model. But the biggest revolution was the one from recurrent neural networks to transformers. Recurrent neural networks processed language sequentially, token (e.g., word) by token. Since the revolutionary paper \" Attention is all you need\" transformers have become the golden standards by capturing context efficiently. Compared to previous models transformers were able to:\n",
    "        - processing text in parallel\n",
    "        - capture long text dependencies through context\n",
    "    - Transformers \n",
    "        - Word-based transformers\n",
    "        - Sentence-based transformers\n",
    "- What are embeddings\n",
    "    -  Embeddings \n",
    "- How can we use embeddings for scale development\n",
    "    - The unexpected twist \"using sentence embeddings as response vectors\" [ Mata and Guenole] \n",
    "\n",
    "Large Language Models are one of the most exciting scientific innovation of the last decade and are becoming ubiquitous, from chatbots (chatGPT) to..... LLMs are defined as  a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification, next word prediction etc.(wiki)\n",
    "\n",
    "These models can have different type of architectures depending on the task they are developed to. For instance, if one wants to produce text a decoder model would preferred (such as GPT). For LLMEP we will be focusing mostly on analyze text, which is better achieved using encoder models. You can think of an encoder as a transformation tool that takes text and transforms it into a multidimensional numerical vector, also called embedding. (maybe add text about the transformation to explain more clearly)\n",
    "\n",
    "The most popular architecture for encode model is that of Bidirectional Encoder Representation from Transformers ([BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)). BERT models are extremely good in understading words due to their ability to capture the word-context through \"looking\" both at the text left and right of the word. However, when we deal with questions from a survey, they are not formed from a single word but entire sentences, which is why we need models that can transform and analyze entire sentences, such as Sentence-BERT (SBERT).\n",
    "\n",
    "Before SBERT one had to aggregate (through max or mean pooling) word-level embeddings in order to obtain sentence embeddings. The problem with this approach was that the semantic meaning of sentences would be hard to capture. For instance....\n",
    "SBERT models learn the semantic meaning of sentences through the [addition of siamese and triplet networks () in its architecture](https://towardsdatascience.com/sbert-deb3d4aef8a4).\n",
    "{The idea was to first create sentence embeddings through max/mean pooling for two sentences, and concatenate these embeddings with another embedding obtained from their difference. Further, this concatenated embedding was used in a classification task to predict whether the sentences were entailing each other, opposing each other or neutral to each other.} \n",
    "\n",
    "SBERT-based models capture the meaning of sentences much better than BERT-based models allowing us to transform sentences, such as items in a questionnaire, into item embeddings which we can in turn use for psychometric analyses. \n",
    "\n",
    "But before taking our next step into embedding psychometrics let's demonstrate all that we have discussed so far.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GPT is a decoder mode, which takes the input and outputs text, \n",
    "\n",
    "Most of these models comprises an [encoder and a decoder layer](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder). You can think of an encoder as a transformation tool that takes text and transforms it into a multidimensional numerical vector, also called embedding. Each of the dimensions of this text embedding captures an aspect of that text. For example, one dimension may capture whether that text is about someone or something, whereas another dimension would represent if that text is joyful or sad etc. Then, the decoder would take this vector and, using its own transformational properties, generate the desired output (e.g., translated text). \n",
    "For en\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of the blog post\n",
    "\n",
    "Situation\n",
    "- self-report measures are one of the most used tools to evaluate peoples' opinion beliefs needs, cognitive abilities etc\n",
    "\n",
    "Complication\n",
    "Developing good measures is \n",
    "- Expense and Resources: Detail the cost implications, including the need for large sample sizes and extensive data collection efforts.\n",
    "- Time Consumption: Elaborate on the time required to review existing literature, develop questions, pilot tests, and analyze data.\n",
    "- Risk of Failure:Discuss the risks of poorly designed measures, including respondent fatigue, biased answers, and unreliable data.\n",
    "- Limitations in Testing Explain the practical limitations of testing numerous questions and variations, and how this increases the risk of introducing biases.\n",
    "\n",
    "Question\n",
    "How can we speed-up and simplify scale development so that, when we use self-report measures, we are quite sure they work well?\n",
    "\n",
    "Solution\n",
    "Embedding psychometrics!\n",
    "What is embedding psychometrics:....\n",
    "    and what is sentence embeddings to begin with...\n",
    "\n",
    "- Embedding psychometrics is cheap: it does not require extensive data collection, large sample size as it is simply done through leveraging the power of pre-trained llms\n",
    "- Embedding psychometrics is fast: surely questions need to be reviewed and collected, but one can simply pilot existing questions all at once and analyze the data in real time without needing to wait for data collection.\n",
    "- Embedding psychometrics fails fast\n",
    "- Embedding psychometrics not bias free but less bias sensitive \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is embedding Psychometrics\n",
    "    How is it different from classic psychometrics\n",
    "\n",
    "Why embedding psychometrics?\n",
    "\n",
    "How? \n",
    "\n",
    "\n",
    "Classic Psychometrics\n",
    "- Research question\n",
    "Embedding Psychometrics\n",
    "\n",
    "\n",
    "Structure of the blopost\n",
    "- E"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
